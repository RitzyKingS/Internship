{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3128bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec0d5a",
   "metadata": {},
   "source": [
    "### 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08196f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "def scrape_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    headers_text = [header.text for header in headers]\n",
    "    df = pd.DataFrame({'Header': headers_text})\n",
    "    return df\n",
    "\n",
    "df = scrape_wikipedia_headers('https://en.wikipedia.org/wiki/Main_Page')\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498dc97b",
   "metadata": {},
   "source": [
    "### 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134fbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_former_presidents(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the div elements with class \"presidentListing\"\n",
    "    divs = soup.find_all('div', class_='presidentListing')\n",
    "\n",
    "    # Initialize empty lists to store the data\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    # Iterate over the div elements\n",
    "    for div in divs:\n",
    "        # Extract the name and term of office\n",
    "        name = div.find('h3').text.strip()\n",
    "        term = div.find('p').text.strip()\n",
    "\n",
    "        # Append the data to the respective lists\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "\n",
    "    # Create a pandas DataFrame with the extracted data\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36a5bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Name  \\\n",
      "0           Shri Ram Nath Kovind (birth - 1945)   \n",
      "1             Shri Pranab Mukherjee (1935-2020)   \n",
      "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
      "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
      "4            Shri K. R. Narayanan (1920 - 2005)   \n",
      "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
      "6               Shri R Venkataraman (1910-2009)   \n",
      "7                  Giani Zail Singh (1916-1994)   \n",
      "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
      "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
      "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
      "11                 Dr. Zakir Husain (1897-1969)   \n",
      "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
      "13              Dr. Rajendra Prasad (1884-1963)   \n",
      "\n",
      "                                       Term of Office  \n",
      "0      Term of Office: 25 July, 2017 to 25 July, 2022  \n",
      "1      Term of Office: 25 July, 2012 to 25 July, 2017  \n",
      "2      Term of Office: 25 July, 2007 to 25 July, 2012  \n",
      "3      Term of Office: 25 July, 2002 to 25 July, 2007  \n",
      "4      Term of Office: 25 July, 1997 to 25 July, 2002  \n",
      "5      Term of Office: 25 July, 1992 to 25 July, 1997  \n",
      "6      Term of Office: 25 July, 1987 to 25 July, 1992  \n",
      "7      Term of Office: 25 July, 1982 to 25 July, 1987  \n",
      "8      Term of Office: 25 July, 1977 to 25 July, 1982  \n",
      "9   Term of Office: 24 August, 1974 to 11 February...  \n",
      "10  Term of Office: 3 May, 1969 to 20 July, 1969 a...  \n",
      "11        Term of Office: 13 May, 1967 to 3 May, 1969  \n",
      "12       Term of Office: 13 May, 1962 to 13 May, 1967  \n",
      "13   Term of Office: 26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "presidents_df = scrape_former_presidents(url)\n",
    "\n",
    "if presidents_df is not None:\n",
    "    print(presidents_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe6b28",
   "metadata": {},
   "source": [
    "### 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#### **a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.**\n",
    "#### b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "\n",
    "#### c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4dd7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Men's Cricket:\n",
      "           Team Matches Points Rating\n",
      "0     Australia      23   2714    118\n",
      "1      Pakistan      20   2316    116\n",
      "2         India      33   3807    115\n",
      "3   New Zealand      27   2806    104\n",
      "4       England      24   2426    101\n",
      "5  South Africa      19   1910    101\n",
      "6    Bangladesh      25   2451     98\n",
      "7     Sri Lanka      28   2378     85\n",
      "8   Afghanistan      13   1067     82\n",
      "9   West Indies      32   2201     69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "def scrape_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].find('span', class_='u-hide-phablet').text.strip()\n",
    "        match = columns[2].text.strip()\n",
    "        point = columns[3].text.strip().replace(',', '')\n",
    "        rating = columns[4].text.strip()\n",
    "\n",
    "        teams.append(team)\n",
    "        matches.append(match)\n",
    "        points.append(point)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrape_odi_players(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        player_element = columns[1].find('span', class_='u-hide-phablet')\n",
    "        player = player_element.text.strip() if player_element else ''\n",
    "        team_element = columns[2].find('span', class_='u-hide-phablet')\n",
    "        team = team_element.text.strip() if team_element else ''\n",
    "        rating = columns[4].text.strip()\n",
    "\n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "odi_teams_df = scrape_odi_teams(teams_url)\n",
    "\n",
    "\n",
    "print(\"Top 10 ODI Teams in Men's Cricket:\")\n",
    "print(odi_teams_df.head(10))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7e885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Batsmen:\n",
      "                  Player Team Rating\n",
      "0                                886\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2           Fakhar Zaman  PAK    755\n",
      "3            Imam-ul-Haq  PAK    745\n",
      "4           Shubman Gill  IND    738\n",
      "5           Harry Tector  IRE    726\n",
      "6           David Warner  AUS    726\n",
      "7            Virat Kohli  IND    719\n",
      "8        Quinton de Kock   SA    718\n",
      "9           Rohit Sharma  IND    707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "\n",
    "def scrape_odi_players(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        player_element = columns[1].find('a')\n",
    "        player = player_element.text.strip() if player_element else ''\n",
    "        team_element = columns[2].find('span', class_='table-body__logo-text')\n",
    "        team = team_element.text.strip() if team_element else ''\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_batsmen_df = scrape_odi_players(batsmen_url)\n",
    "\n",
    "print(\"Top 10 Men's ODI Batsmen:\")\n",
    "print(odi_batsmen_df.head(10))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b77d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Men's ODI Bowlers:\n",
      "             Player Team Rating\n",
      "0                           705\n",
      "1    Mohammed Siraj  IND    691\n",
      "2    Mitchell Starc  AUS    686\n",
      "3        Matt Henry   NZ    667\n",
      "4       Trent Boult   NZ    660\n",
      "5        Adam Zampa  AUS    652\n",
      "6       Rashid Khan  AFG    640\n",
      "7    Shaheen Afridi  PAK    630\n",
      "8  Mujeeb Ur Rahman  AFG    630\n",
      "9     Mohammad Nabi  AFG    626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "def scrape_odi_players(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        player_element = columns[1].find('a')\n",
    "        player = player_element.text.strip() if player_element else ''\n",
    "        team_element = columns[2].find('span', class_='table-body__logo-text')\n",
    "        team = team_element.text.strip() if team_element else ''\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "odi_bowlers_df = scrape_odi_players(bowlers_url)\n",
    "\n",
    "\n",
    "print(\"Top 10 Men's ODI Bowlers:\")\n",
    "print(odi_bowlers_df.head(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e6bdb",
   "metadata": {},
   "source": [
    "### 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#### a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#### b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d843eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Women's Cricket:\n",
      "           Team Matches Points Rating\n",
      "0     Australia      21   3603    172\n",
      "1       England      28   3342    119\n",
      "2  South Africa      26   3098    119\n",
      "3         India      27   2820    104\n",
      "4   New Zealand      28   2688     96\n",
      "5   West Indies      29   2743     95\n",
      "6    Bangladesh      14    977     70\n",
      "7     Sri Lanka      12    820     68\n",
      "8      Thailand      12    806     67\n",
      "9      Pakistan      27   1678     62\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "             Player Team Rating\n",
      "0                           758\n",
      "1       Beth Mooney  AUS    754\n",
      "2   Laura Wolvaardt   SA    732\n",
      "3    Natalie Sciver  ENG    731\n",
      "4       Meg Lanning  AUS    717\n",
      "5  Harmanpreet Kaur  IND    716\n",
      "6   Smriti Mandhana  IND    714\n",
      "7      Ellyse Perry  AUS    626\n",
      "8   Stafanie Taylor   WI    618\n",
      "9    Tammy Beaumont  ENG    595\n",
      "\n",
      "Top 10 Women's ODI All-rounders:\n",
      "             Player Team Rating\n",
      "0                           382\n",
      "1    Natalie Sciver  ENG    371\n",
      "2      Ellyse Perry  AUS    366\n",
      "3    Marizanne Kapp   SA    349\n",
      "4       Amelia Kerr   NZ    328\n",
      "5     Deepti Sharma  IND    322\n",
      "6  Ashleigh Gardner  AUS    292\n",
      "7     Jess Jonassen  AUS    250\n",
      "8     Sophie Devine   NZ    233\n",
      "9          Nida Dar  PAK    232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_odi_teams_women(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:11]:  # Limit to the top 10 teams\n",
    "        columns = row.find_all('td')\n",
    "        team_element = columns[1].find('span', class_='u-hide-phablet')\n",
    "        team = team_element.text.strip() if team_element else ''\n",
    "        match = columns[2].text.strip()\n",
    "        point = columns[3].text.strip().replace(',', '')\n",
    "        rating = columns[4].text.strip()\n",
    "\n",
    "        teams.append(team)\n",
    "        matches.append(match)\n",
    "        points.append(point)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrape_odi_batting_players_women(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:11]:  # Limit to the top 10 players\n",
    "        columns = row.find_all('td')\n",
    "        player_element = columns[1].find('a')\n",
    "        player = player_element.text.strip() if player_element else ''\n",
    "        team_element = columns[2].find('span', class_='table-body__logo-text')\n",
    "        team = team_element.text.strip() if team_element else ''\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrape_odi_allrounders_women(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in rows[1:11]:  # Limit to the top 10 players\n",
    "        columns = row.find_all('td')\n",
    "        player_element = columns[1].find('a')\n",
    "        player = player_element.text.strip() if player_element else ''\n",
    "        team_element = columns[2].find('span', class_='table-body__logo-text')\n",
    "        team = team_element.text.strip() if team_element else ''\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "teams_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "odi_teams_women_df = scrape_odi_teams_women(teams_url)\n",
    "\n",
    "batting_players_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "odi_batting_players_women_df = scrape_odi_batting_players_women(batting_players_url)\n",
    "\n",
    "allrounders_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "odi_allrounders_women_df = scrape_odi_allrounders_women(allrounders_url)\n",
    "\n",
    "print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "print(odi_teams_women_df)\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Women's ODI Batting Players:\")\n",
    "print(odi_batting_players_women_df)\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Women's ODI All-rounders:\")\n",
    "print(odi_allrounders_women_df)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d8546",
   "metadata": {},
   "source": [
    "### 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "##### i) Headline\n",
    "##### ii) Time\n",
    "##### iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec61685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Details:\n",
      "Headline: Stocks tumble on Friday, notching weekly losses, as traders' rate hike fears return: Live updates\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/06/stock-market-today-live-updates.html\n",
      "\n",
      "Headline: Ukraine reports advances near eastern city of Bakhmut; Germany opposes sending cluster bombs to Kyiv\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/07/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html\n",
      "\n",
      "Headline: Wagner leader Prigozhin in St. Petersburg, Belarus leader says; Zelenskyy calls for more weapons\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/06/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html\n",
      "\n",
      "Headline: Zelenskyy warns of provocations at nuclear plant; Xi reportedly warned Putin against nuclear attack\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/05/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html\n",
      "\n",
      "Headline: Wagner's Prigozhin reportedly resurfaces; NATO extends Stoltenberg's term\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/04/russia-ukraine-war-updates-for-july-4-2023.html\n",
      "\n",
      "Headline: Russia's Medvedev cites risks of nuclear war; Moscow calls loss of Wagner forces 'no threat'\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/03/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html\n",
      "\n",
      "Headline: 'We are in uncharted territory': World records hottest day for the third time in just four days\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/07/climate-world-records-hottest-day-ever-for-the-third-time-this-week.html\n",
      "\n",
      "Headline: Bad news for nervous flyers: Turbulence is getting worse as the planet warms\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/06/bad-news-for-nervous-flyers-turbulence-is-getting-worse-as-the-planet-warms.html\n",
      "\n",
      "Headline: World registers hottest day since records began — with fresh highs expected\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/05/climate-crisis-world-registers-hottest-day-since-records-began.html\n",
      "\n",
      "Headline: Millions of workers face up to challenge of heat stress and productivity losses\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/05/as-planet-heats-calls-to-protect-workers-grow-louder.html\n",
      "\n",
      "Headline: El Niño has officially begun. UN says phenomenon likely to threaten lives\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/04/el-nio-un-says-climate-phenomenon-likely-to-break-temperature-records.html\n",
      "\n",
      "Headline: Southeast Asia's IPO market is an investor favorite amid global headwinds, Deloitte says\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/07/southeast-asias-ipo-market-an-investor-favorite-amid-global-headwinds-deloitte.html\n",
      "\n",
      "Headline: We the see biggest opportunities in Indonesia, says DP World\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/video/2023/06/26/we-the-see-biggest-opportunities-in-indonesia-says-dubai-based-supply-chain-firm.html\n",
      "\n",
      "Headline: Singapore pledged billions to fight climate change. Experts say it's not enough\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/06/21/singapore-climate-change-green-bonds-blended-financing-needed.html\n",
      "\n",
      "Headline: V3 Gourmet explains why it chose Singapore for Bacha Coffee's flagship store\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/video/2023/06/19/v3-gourmet-discusses-bacha-coffee-singapore-flagship-store.html\n",
      "\n",
      "Headline: Revenues of 'warung' operators grew after they signed up with us: Bukalapak\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/video/2023/06/12/bukalapak-discusses-how-its-helping-warung-operators.html\n",
      "\n",
      "Headline: The best places to eat in Andalusia — from a chef with three Michelin stars\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/06/the-best-places-to-eat-in-andalusia-from-chef-angel-leon.html\n",
      "\n",
      "Headline: What a 250,000 euro penthouse renovation in Barcelona looks like\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/06/26/real-estate-in-barcelona-what-a-250k-euro-renovation-looks-like.html\n",
      "\n",
      "Headline: Serving 'lunch' before midnight — and other ways airlines can reduce jet lag\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/06/22/how-to-reduce-jet-lag-on-long-flights-start-on-the-plane-says-qantas.html\n",
      "\n",
      "Headline: These workers take ‘hush trips.’ Here’s how they’re hiding them from the boss\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/06/20/hush-trips-heres-how-workers-are-hiding-them-from-their-employers.html\n",
      "\n",
      "Headline: Michelin Guide adds 17 food stalls in Singapore to its Bib Gourmand list\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/06/16/michelin-guide-adds-17-singapore-food-stalls-to-its-bib-gourmand-list.html\n",
      "\n",
      "Headline: How to start a pet-sitting side hustle—you could make as much as $40 per hour\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/08/how-to-start-a-side-hustle-as-a-pet-sitter-.html\n",
      "\n",
      "Headline: These U.S. states have the best and the worst tap water\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/08/best-and-worst-tap-water-states-j-d-power-study.html\n",
      "\n",
      "Headline: The No. 1 vitamin kids in the U.S. 'aren't getting enough of' today: Dietitian and parenting expert\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/08/no-1-nutrient-vitamin-american-kids-today-arent-getting-enough-of-according-to-dietitian-parenting-expert.html\n",
      "\n",
      "Headline: The 10 U.S. cities where rent has increased the most in the last year\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/08/us-cities-where-rent-has-increased-the-most.html\n",
      "\n",
      "Headline: Justin Bieber's Bored Ape NFT has lost about 95% of its value since 2022\n",
      "Time: \n",
      "News Link: https://www.cnbc.comhttps://www.cnbc.com/2023/07/07/justin-biebers-bored-ape-nft-has-lost-95-percent-of-its-value-since-2022.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_news_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "    links = []\n",
    "\n",
    "    for article in articles:\n",
    "        headline_element = article.find('a')\n",
    "        if headline_element:\n",
    "            headline = headline_element.text.strip()\n",
    "        else:\n",
    "            headline = ''\n",
    "\n",
    "        time_element = article.find('time', class_='Card-time')\n",
    "        if time_element:\n",
    "            time = time_element.text.strip()\n",
    "        else:\n",
    "            time = ''\n",
    "\n",
    "        link_element = article.find('a')\n",
    "        if link_element:\n",
    "            link = 'https://www.cnbc.com' + link_element['href']\n",
    "        else:\n",
    "            link = ''\n",
    "\n",
    "        headlines.append(headline)\n",
    "        times.append(time)\n",
    "        links.append(link)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Headline': headlines,\n",
    "        'Time': times,\n",
    "        'News Link': links\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "news_df = scrape_news_details(url)\n",
    "\n",
    "print(\"News Details:\")\n",
    "for _, row in news_df.iterrows():\n",
    "    print(f\"Headline: {row['Headline']}\")\n",
    "    print(f\"Time: {row['Time']}\")\n",
    "    print(f\"News Link: {row['News Link']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797b5a4",
   "metadata": {},
   "source": [
    "### 6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#### Scrape below mentioned details and make data frame\n",
    "#### i) Paper Title\n",
    "#### ii) Authors\n",
    "#### iii) Published Date\n",
    "#### iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e83672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Downloaded Articles in the Last 90 Days:\n",
      "paper_title                              authors                                  published_date paper_url\n",
      "                        Reward is enough [David Silver, Satinder Singh, Doina ...   October 2021 None     \n",
      "Explanation in artificial intelligenc...                             [Tim Miller]  February 2019 None     \n",
      "  Creativity and artificial intelligence                      [Margaret A. Boden]    August 1998 None     \n",
      "Conflict-based search for optimal mul... [Guni Sharon, Roni Stern, Ariel Felne...  February 2015 None     \n",
      "Knowledge graphs as tools for explain...         [Ilaria Tiddi, Stefan Schlobach]   January 2022 None     \n",
      "Law and logic: A review from an argum...         [Henry Prakken, Giovanni Sartor]   October 2015 None     \n",
      "Between MDPs and semi-MDPs: A framewo... [Richard S. Sutton, Doina Precup, Sat...    August 1999 None     \n",
      "Explaining individual predictions whe... [Kjersti Aas, Martin Jullum, Anders L... September 2021 None     \n",
      "Multiple object tracking: A literatur...   [Wenhan Luo, Junliang Xing and 4 more]     April 2021 None     \n",
      "A survey of inverse reinforcement lea...          [Saurabh Arora, Prashant Doshi]    August 2021 None     \n",
      "Evaluating XAI: A comparison of rule-... [Jasper van der Waa, Elisabeth Nieuwb...  February 2021 None     \n",
      "Explainable AI tools for legal reason... [Joe Collenette, Katie Atkinson, Trev...     April 2023 None     \n",
      " Hard choices in artificial intelligence [Roel Dobbe, Thomas Krendl Gilbert, Y...  November 2021 None     \n",
      "Assessing the communication gap betwe... [Oskar Wysocki, Jessica Katharine Dav...     March 2023 None     \n",
      "Explaining black-box classifiers usin... [Eoin M. Kenny, Courtney Ford, Molly ...       May 2021 None     \n",
      "The Hanabi challenge: A new frontier ... [Nolan Bard, Jakob N. Foerster and 13...     March 2020 None     \n",
      "   Wrappers for feature subset selection             [Ron Kohavi, George H. John]  December 1997 None     \n",
      "Artificial cognition for social human... [Séverin Lemaignan, Mathieu Warnier a...      June 2017 None     \n",
      "A review of possible effects of cogni... [Tomáš Kliegr, Štěpán Bahník, Johanne...      June 2021 None     \n",
      "The multifaceted impact of Ada Lovela...                 [Luigia Carlucci Aiello]      June 2016 None     \n",
      "Robot ethics: Mapping the issues for ... [Patrick Lin, Keith Abney, George Bekey]     April 2011 None     \n",
      "Reward (Mis)design for autonomous dri... [W. Bradley Knox, Alessandro Allievi ...     March 2023 None     \n",
      "Planning and acting in partially obse... [Leslie Pack Kaelbling, Michael L. Li...       May 1998 None     \n",
      "What do we want from Explainable Arti... [Markus Langer, Daniel Oster and 6 more]      July 2021 None     \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(time_period):\n",
    "    \"\"\"Scrape the details of most downloaded articles from AI in the last `time_period` days.\"\"\"\n",
    "\n",
    "    url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    for li in soup.find_all(\"li\", class_=\"sc-9zxyh7-1\"):\n",
    "        article_details = {}\n",
    "        article = li.find(\"article\")\n",
    "        \n",
    "\n",
    "        title_element = article.find(\"h2\", class_=\"sc-1qrq3sd-1\")\n",
    "        article_details[\"paper_title\"] = title_element.text.strip() if title_element else \"\"\n",
    "        article_details[\"paper_url\"] = title_element.get(\"href\")\n",
    "\n",
    "\n",
    "\n",
    "        authors_element = article.find(\"span\", class_=\"sc-1w3fpd7-0\")\n",
    "        article_details[\"authors\"] = authors_element.text.strip().split(\", \") if authors_element else []\n",
    "        date_element = article.find(\"span\", class_=\"sc-1thf9ly-2\")\n",
    "        article_details[\"published_date\"] = date_element.text.strip() if date_element else \"\"\n",
    "\n",
    "        articles.append(article_details)\n",
    "\n",
    "    return articles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    articles = scrape_most_downloaded_articles(90)\n",
    "    df = pd.DataFrame(articles, columns=[\"paper_title\", \"authors\", \"published_date\", \"paper_url\"])\n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", 60)  # Adjust column width\n",
    "    print(\"Most Downloaded Articles in the Last 90 Days:\")\n",
    "    print(df.to_string(index=False, justify=\"left\", max_colwidth=40, na_rep=\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a374acd",
   "metadata": {},
   "source": [
    "### 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "#### i) Restaurant name\n",
    "#### ii) Cuisine\n",
    "#### iii) Location\n",
    "#### iv) Ratings\n",
    "#### v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5b88d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Details:\n",
      "                         Restaurant Name  \\\n",
      "0     The Bier Library Brewery & Kitchen   \n",
      "1                     The Bangalore Cafe   \n",
      "2                         Hard Rock Cafe   \n",
      "3                          Spice Terrace   \n",
      "4                   Skydeck By Sherlocks   \n",
      "5                                 Mirage   \n",
      "6                               Badmaash   \n",
      "7                             JW Kitchen   \n",
      "8                             Farzi Cafe   \n",
      "9                           Uru Brewpark   \n",
      "10                               Magique   \n",
      "11                                  Zest   \n",
      "12                              Hammered   \n",
      "13                               Toscano   \n",
      "14  Salt - Indian Restaurant Bar & Grill   \n",
      "15                        The Biere Club   \n",
      "16                                 Raahi   \n",
      "17                                 Shiro   \n",
      "18                         Chutney Chang   \n",
      "19                               Sanchez   \n",
      "20                           Cafe Azzure   \n",
      "\n",
      "                                                               Cuisine  \\\n",
      "0      ₹ 1,900 for 2 (approx) | Continental, Finger Food, North Indian   \n",
      "1          ₹ 800 for 2 (approx) | Continental, North Indian, Fast Food   \n",
      "2          ₹ 2,500 for 2 (approx) | Continental, American, Finger Food   \n",
      "3                       ₹ 3,900 for 2 (approx) | North Indian, Mughlai   \n",
      "4                       ₹ 1,400 for 2 (approx) | North Indian, Chinese   \n",
      "5                    ₹ 2,800 for 2 (approx) | Asian, Chinese, Desserts   \n",
      "6    ₹ 1,800 for 2 (approx) | North Indian, Chettinad, Andhra, Biryani   \n",
      "7                   ₹ 2,200 for 2 (approx) | North Indian, Continental   \n",
      "8                               ₹ 1,600 for 2 (approx) | Modern Indian   \n",
      "9   ₹ 2,000 for 2 (approx) | North Indian, Italian, Continental, Asian   \n",
      "10                               ₹ 1,500 for 2 (approx) | North Indian   \n",
      "11         ₹ 1,200 for 2 (approx) | North Indian, Chinese, Continental   \n",
      "12         ₹ 2,000 for 2 (approx) | Finger Food, Italian, North Indian   \n",
      "13                                    ₹ 1,600 for 2 (approx) | Italian   \n",
      "14        ₹ 1,200 for 2 (approx) | North Indian, Mughlai, South Indian   \n",
      "15                                ₹ 2,000 for 2 (approx) | Finger Food   \n",
      "16                                     ₹ 1,900 for 2 (approx) | Fusion   \n",
      "17                            ₹ 3,000 for 2 (approx) | Japanese, Asian   \n",
      "18                      ₹ 1,500 for 2 (approx) | Chinese, North Indian   \n",
      "19                                    ₹ 1,800 for 2 (approx) | Mexican   \n",
      "20    ₹ 1,200 for 2 (approx) | Continental, Italian, Desserts, Bengali   \n",
      "\n",
      "                                                   Location Ratings Image URL  \n",
      "0                              Koramangala, South Bangalore                    \n",
      "1                           Shanti Nagar, Central Bangalore                    \n",
      "2                         St. Marks Road, Central Bangalore                    \n",
      "3   JW Marriott Hotel,Vittal Mallya Road, Central Bangalore                    \n",
      "4                                MG Road, Central Bangalore                    \n",
      "5                            Ashok Nagar, Central Bangalore                    \n",
      "6             UB City,Vittal Mallya Road, Central Bangalore                    \n",
      "7   JW Marriott Hotel,Vittal Mallya Road, Central Bangalore                    \n",
      "8             UB City,Vittal Mallya Road, Central Bangalore                    \n",
      "9                                 JP Nagar, South Bangalore                    \n",
      "10                                 Ejipura, South Bangalore                    \n",
      "11                          Lavelle Road, Central Bangalore                    \n",
      "12                       Cunningham Road, Central Bangalore                    \n",
      "13                          Lavelle Road, Central Bangalore                    \n",
      "14                               UB City, Central Bangalore                    \n",
      "15                    Vittal Mallya Road, Central Bangalore                    \n",
      "16                        St. Marks Road, Central Bangalore                    \n",
      "17            UB City,Vittal Mallya Road, Central Bangalore                    \n",
      "18    HM Eleganza Building,Church Street, Central Bangalore                    \n",
      "19            UB City,Vittal Mallya Road, Central Bangalore                    \n",
      "20                               MG Road, Central Bangalore                    \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_restaurants():\n",
    "    url = \"https://www.dineout.co.in/bangalore-restaurants\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    restaurants = []\n",
    "    for card in soup.find_all(\"div\", class_=\"restnt-card\"):\n",
    "        restaurant_details = {}\n",
    "\n",
    "        name_element = card.find(\"div\", class_=\"restnt-info\")\n",
    "        restaurant_details[\"Restaurant Name\"] = name_element.find(\"a\").text.strip()\n",
    "\n",
    "\n",
    "        cuisine_element = card.find(\"span\", class_=\"double-line-ellipsis\")\n",
    "        restaurant_details[\"Cuisine\"] = cuisine_element.text.strip() if cuisine_element else \"\"\n",
    "\n",
    "\n",
    "        location_element = card.find(\"div\", class_=\"restnt-loc ellipsis\")\n",
    "        restaurant_details[\"Location\"] = location_element.text.strip() if location_element else \"\"\n",
    "\n",
    "\n",
    "        ratings_element = card.find(\"span\", class_=\"ratings-val\")\n",
    "        restaurant_details[\"Ratings\"] = ratings_element.text.strip() if ratings_element else \"\"\n",
    "\n",
    "\n",
    "        image_element = card.find(\"div\", class_=\"restnt-img\")\n",
    "        restaurant_details[\"Image URL\"] = image_element.find(\"img\")[\"data-src\"] if image_element else \"\"\n",
    "\n",
    "        restaurants.append(restaurant_details)\n",
    "\n",
    "    return restaurants\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    restaurants = scrape_dineout_restaurants()\n",
    "    df = pd.DataFrame(restaurants, columns=[\"Restaurant Name\", \"Cuisine\", \"Location\", \"Ratings\", \"Image URL\"])\n",
    "    print(\"Restaurant Details:\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294263b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
