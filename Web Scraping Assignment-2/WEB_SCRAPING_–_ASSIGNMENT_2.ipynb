{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: Scraping Data Analyst Jobs in Bangalore**"
      ],
      "metadata": {
        "id": "lfiV4B69zBx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jlK9no9y62_",
        "outputId": "050ae68a-2ac5-48a6-d1b9-a432578bf048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Get the webpage\n",
        "url = \"https://www.naukri.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Step 2-4: Enter search criteria and click search\n",
        "search_params = {\n",
        "    \"skill\": \"Data Analyst\",\n",
        "    \"location\": \"Bangalore\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=search_params)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Step 5: Scrape data for first 10 jobs\n",
        "job_listings = soup.find_all(\"article\", class_=\"jobTuple\")\n",
        "\n",
        "data = []\n",
        "for job in job_listings[:10]:\n",
        "    title = job.find(\"a\", class_=\"title\").text.strip()\n",
        "    location = job.find(\"li\", class_=\"location\").text.strip()\n",
        "    company_name = job.find(\"a\", class_=\"subTitle\").text.strip()\n",
        "    experience = job.find(\"li\", class_=\"experience\").text.strip()\n",
        "    data.append({\"Title\": title, \"Location\": location, \"Company\": company_name, \"Experience\": experience})\n",
        "\n",
        "# Step 6: Create a dataframe\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: Scraping Data Scientist Jobs in Bangalore**"
      ],
      "metadata": {
        "id": "Anb242p7zH1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar to Q1, adjust search_params for Data Scientist\n",
        "search_params = {\n",
        "    \"skill\": \"Data Scientist\",\n",
        "    \"location\": \"Bangalore\"\n",
        "}\n",
        "\n",
        "# Rest of the code remains the same\n"
      ],
      "metadata": {
        "id": "8rC_dJ29y-dM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: Scraping Data Scientist Jobs in Delhi/NCR with Salary Filter**"
      ],
      "metadata": {
        "id": "cGL8TdjTzMcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar to Q1, adjust search_params for Delhi/NCR and apply salary filter\n",
        "search_params = {\n",
        "    \"skill\": \"Data Scientist\",\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=search_params)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Apply filters manually (you can automate this if the website structure allows)\n",
        "# Then scrape the data similarly as before\n"
      ],
      "metadata": {
        "id": "RmSZHNWazKjr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4: Scraping Sunglasses Listings on Flipkart**"
      ],
      "metadata": {
        "id": "w7ZxqrlyzThd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get the webpage\n",
        "url = \"https://www.flipkart.com/\"\n",
        "\n",
        "# Enter search query for sunglasses\n",
        "search_params = {\n",
        "    \"q\": \"sunglasses\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=search_params)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Step 2: Scrape data for first 100 sunglasses\n",
        "sunglasses = soup.find_all(\"div\", class_=\"_2kHMtA\")\n",
        "\n",
        "data = []\n",
        "for sunglass in sunglasses[:100]:\n",
        "    brand = sunglass.find(\"div\", class_=\"_2WkVRV\").text.strip()\n",
        "    description = sunglass.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
        "    price = sunglass.find(\"div\", class_=\"_30jeq3\").text.strip()\n",
        "    data.append({\"Brand\": brand, \"Description\": description, \"Price\": price})\n",
        "\n",
        "# Step 3: Create a dataframe\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FevIHL0PzPk1",
        "outputId": "0bf9d6c9-7db2-467f-844c-1eafe04736c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6: Scraping Sneakers Listings on Flipkart**"
      ],
      "metadata": {
        "id": "e32PW2_izZ4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar to Q4, adjust search query for sneakers\n",
        "search_params = {\n",
        "    \"q\": \"sneakers\"\n",
        "}\n",
        "\n",
        "# Rest of the code remains the same\n"
      ],
      "metadata": {
        "id": "th6B4XIxzWPV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lOniYS89zcyt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}